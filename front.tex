\begin{dedication}
  This is all a bit pointless.
\end{dedication}

\begin{acknowledgments}
  \needswork
  % cvpr2013
  This work was supported, in part, by AFOSR FA9550-11-1-0327, NSF IIS-1017199
  and ARL under Cooperative Agreement Number W911NF-10-2-0060 (DARPA Mind's Eye).

  % acs2012a
  This work was supported, in part, by NSF Grant No. CCF-0438806, NRL Contract No.
  N00173-10-1-G023, ARL Cooperative Agreement No. W911NF-10-2-0060, and the Rosen
  Center for Advanced Computing.
  %
  Any views or conclusions expressed in this document are those of the authors
  and do not necessarily reflect or represent the views or official policies,
  expressed or implied, of NSF, NRL, ONR, ARL, or the U.S. Government.
  %
  The U.S. Government is authorized to reproduce and distribute reprints for
  Government purposes, notwithstanding any copyright notation herein.

  % icra2011
  This work was supported, in part, by NSF grant CCF-0438806, by the Naval
  Research Laboratory under Contract Number N00173-10-1-G023, by the Army
  Research Laboratory accomplished under Cooperative Agreement Number
  W911NF-10-2-0060, and by computational resources provided by Information
  Technology at Purdue through its Rosen Center for Advanced Computing.
  %
  Any views, opinions, findings, conclusions, or recommendations contained or
  expressed in this document or material are those of the author(s) and do not
  necessarily reflect or represent the views or official policies, either
  expressed or implied, of NSF, the Naval Research Laboratory, the Office of
  Naval Research, the Army Research Laboratory, or the U.S. Government.
  %
  The U.S. Government is authorized to reproduce and distribute reprints for
  Government purposes, notwithstanding any copyright notation herein.\@

  % nips2013e
  This research was sponsored by the Army Research Laboratory and was
  accomplished under Cooperative Agreement Number W911NF-10-2-0060.
  %
  The views and conclusions contained in this document are those of the authors
  and should not be interpreted as representing the official policies, either
  express or implied, of the Army Research Laboratory or the U.S. Government.
  %
  The U.S. Government is authorized to reproduce and distribute reprints for
  Government purposes, notwithstanding any copyright notation herein.

  % nips2013d
  AB, NS, and JMS were supported, in part, by Army Research Laboratory (ARL)
  Cooperative Agreement W911NF-10-2-0060.
  %
  CX and JJC were supported, in part, by ARL Cooperative Agreement
  W911NF-10-2-0062 and NSF CAREER grant IIS-0845282.
  %
  CDF was supported, in part, by NSF grant CNS-0855157.
  %
  CH and SJH were supported, in part, by the McDonnell Foundation.
  %
  BAP was supported, in part, by Science Foundation Ireland grant 09/IN.1/I2637.
  %
  The views and conclusions contained in this document are those of the authors
  and should not be interpreted as representing the official policies, either
  express or implied, of the supporting institutions.
  %
  The U.S. Government is authorized to reproduce and distribute reprints for
  Government purposes, notwithstanding any copyright notation herein.
  %
  Dr.\ Gregory G. Tamer, Jr.\ provided assistance with imaging and analysis.
\end{acknowledgments}

\tableofcontents

\listoftables

\listoffigures

\begin{abstract}
  People often perceive and/or infer things about the world by incorporating
  information available from multiple sources in the context of world
  knowledge.
  %
  This ability to integrate information across modalities is the focus of my
  research, especially, the integration between vision and language.
  %
  I endeavour to build systems that exhibit this ability, and in the process,
  gain further insight into the processes of perception and inference as
  exhibited by humans.

  One line of research involves reasoning about the physical structure of
  composable entities, involving integration within vision, and across vision
  and language through the medium of robotics.
  %
  I show how even information extracted from unreliable sources, such as
  feature detectors, in the presence of occlusion, is useful when reasoned
  about in the context of other low-level sources, natural-language
  descriptions of the entities, and basic world knowledge.

  Another line of research shows how language, and the compositional structure
  of events and sentences, interplays with the underlying tracking mechanisms
  in video action recognition.
  %
  Such a combined framework allows for video retrieval, video description, and
  focus-of-attention in multi-activity videos.

  I further explore compositionality in a first-of-its-kind study where people
  in an fMRI machine are shown videos depicting activities that correspond to
  unique sentential descriptions.
  %
  By classifying brain-activity with different subsets of the sentential
  structure, first independently and then jointly, I show that brain-activity
  patterns reflect compositionality in sentence structure as the composition of
  independent classifications matches those obtained jointly.
\end{abstract}
